# -*- coding: utf-8 -*-
"""codice_progetto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kJMYlPqMwM1NzDaC-sdE4P3tANUm6RoQ
"""

import pandas as pd
import spacy

mio_dataset= pd.read_csv('/content/preprocessed-50k.csv')

umani = mio_dataset[mio_dataset['source'] == 'human']
macchine = mio_dataset[mio_dataset['source'] == 'ai']

# Campionare 7500 righe casuali da ciascun gruppo
umani_sample = umani.sample(n=7500, random_state=42)
macchine_sample = macchine.sample(n=7500, random_state=42)

# Concatenare i due campioni
dataset_ridotto = pd.concat([umani_sample, macchine_sample])

#pulizia del dataset

import re

nlp = spacy.load('en_core_web_sm')

# Funzione per pulire il testo
def pulisci_testo(text):

    # Rimuovi solo i caratteri speciali (ma non i numeri)
    text = re.sub(r'[^\w\s]', '', text)

    text = re.sub(r'\n', ' ', text)

    # Converte in minuscolo
    text = text.lower()

    # Lemmatizzazione (riduce le parole alla loro forma base)
    doc = nlp(text)
    lemmatized_text = " ".join([token.lemma_ for token in doc if not token.is_stop])

    return lemmatized_text

# Applica la pulizia del testo alla colonna 'Text'
dataset_ridotto['text'] = dataset_ridotto['text'].apply(pulisci_testo)

testi_umano = dataset_ridotto[dataset_ridotto['source'] == 'human'].copy()
testi_macchina = dataset_ridotto[dataset_ridotto['source'] == 'ai'].copy()

#L'uso del metodo .copy() crea una copia indipendente dei dati selezionati
#Questo è importante per evitare problemi di SettingWithCopyWarning in Pandas, che si verificano quando si modifica un sottoinsieme dei dati senza averne fatto una copia esplicita

#feature 1- calcolo il numero di token in ogni frase e il numero di frasi in ogni testo

nlp= spacy.load('en_core_web_sm')
# Funzione per calcolare le lunghezze
def calcola_lunghezza_spacy(testo):
    doc = nlp(testo)
    lunghezza_frasi = len(list(doc.sents))  # Numero di frasi
    numero_token = len(doc)  # Numero di token (parole)
    return lunghezza_frasi, numero_token

# Applico la funzione al dataset di testi prodotti da umano
testi_umano['lunghezza_frasi'], testi_umano['numero_token']= zip(
    *testi_umano['text'].apply(calcola_lunghezza_spacy)
)

#applico anche al secondo dataset
testi_macchina['lunghezza_frasi'], testi_macchina['numero_token'] = zip(
    *testi_macchina['text'].apply(calcola_lunghezza_spacy)
)

from google.colab import files

# Salvo il DataFrame in un file CSV
testi_umano[['lunghezza_frasi', 'numero_token']].to_csv('testi_umano_lunghezze.csv', index=False)
testi_macchina[['lunghezza_frasi', 'numero_token']].to_csv('testi_macchina_lunghezze.csv', index=False)

# Scarico i file sul mio computer
files.download('testi_umano_lunghezze.csv')
files.download('testi_macchina_lunghezze.csv')

# Calcolare la media delle lunghezze per i testi umani
media_umano = testi_umano[['lunghezza_frasi', 'numero_token']].mean()

# Calcolare la media delle lunghezze per i testi generati dalla macchina
media_macchina = testi_macchina[['lunghezza_frasi', 'numero_token']].mean()


# Creare un DataFrame con i risultati delle medie
media_df = pd.DataFrame({
    'Tipo': ['Testi umani', 'Testi macchina'],
    'Media Lunghezza Frasi': [media_umano['lunghezza_frasi'], media_macchina['lunghezza_frasi']],
    'Media Numero Token': [media_umano['numero_token'], media_macchina['numero_token']]
})

# Salvare il DataFrame in un file CSV
media_df.to_csv('media_lunghezze.csv', index=False)

# Scaricare il file CSV
files.download('media_lunghezze.csv')

#grafico per confrontare i valori ottenuti nei testi scritti da macchina e nei testi scritti da umano

import matplotlib.pyplot as plt

import numpy as np

# Valori delle medie
labels = ['Frasi', 'Token']
valori_umano = media_umano.values
valori_macchina = media_macchina.values

# Configurazione del grafico
x = np.arange(len(labels))  # Posizioni per le categorie
width = 0.35  # Larghezza delle barre

# Creazione della figura
fig, ax = plt.subplots(figsize=(8, 5))

# Barre per i testi umani
rects1 = ax.bar(x - width/2, valori_umano, width, label='Testi Umani', color='blue')

# Barre per i testi macchina
rects2 = ax.bar(x + width/2, valori_macchina, width, label='Testi Macchina', color='green')

# Aggiunta delle etichette e del titolo
ax.set_ylabel('Media')
ax.set_title('Confronto del numero di token e numero di frasi tra testi umani e macchina')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()

# Aggiunta dei valori sulle barre
ax.bar_label(rects1, fmt='%.2f', padding=3)
ax.bar_label(rects2, fmt='%.2f', padding=3)

plt.tight_layout()
plt.savefig('grafico_lunghezze.png')
files.download('grafico_lunghezze.png')

#feature 2- varietà del vocabolario
#calcoliamo il Type-Token Ratio (TTR), ossia il rapporto tra il numero di parole uniche (tipi) e il numero totale di parole (token) nel testo

def calcola_ttr(testo):
    parole = testo.split()
    num_token = len(parole)
    num_tipi = len(set(parole))
    ttr = num_tipi / num_token if num_token > 0 else 0
    return ttr

# Applicazione
testi_umano['TTR'] = testi_umano['text'].apply(calcola_ttr)

testi_macchina ['TTR'] = testi_macchina['text'].apply(calcola_ttr)

# Media TTR per i due gruppi
testi_umano_media_ttr= testi_umano['TTR'].mean()
testi_macchina_media_ttr= testi_macchina['TTR'].mean()

# Creazione di un DataFrame con i valori
media_ttr_df = pd.DataFrame({
    'Gruppo': ['Testi umani', 'Testi macchina'],
    'Media TTR': [testi_umano_media_ttr, testi_macchina_media_ttr]
})

# Salvataggio del DataFrame in un file CSV
media_ttr_df.to_csv('media_ttr.csv', index=False)

files.download('media_ttr.csv')

#grafico per confrontare il TTR medio tra testi umani e testi generati dalla macchina
import matplotlib.pyplot as plt

# Dati
labels = ['Testi Umani', 'Testi Macchina']
valori = [testi_umano_media_ttr, testi_macchina_media_ttr]

# Creazione del grafico a barre
plt.figure(figsize=(8, 5))
plt.bar(labels, valori, color=['blue', 'green'])


# Aggiungi il titolo e le etichette
plt.title('Confronto TTR Medio tra Testi Umani e Testi Macchina')
plt.ylabel('TTR Medio')

plt.tight_layout()
plt.savefig('grafico_ttr_medio.png')
files.download('grafico_ttr_medio.png')

#feature 3 - verbi e connettivi
#di questi calcolo la frequenza e il ttr

# Funzione per estrarre i verbi
def estrai_verbi(text):
    doc = nlp(text)
    verbi = [token.text for token in doc if token.pos_ == "VERB"]
    return verbi

# Estrazione dei verbi per i testi umani
verbi_umano = testi_umano['text'].apply(estrai_verbi)

# Estrazione dei verbi per i testi generati dalla macchina
verbi_macchina = testi_macchina['text'].apply(estrai_verbi)

# Funzione per estrarre i connettivi
def estrai_connettivi(text):
    doc = nlp(text)
    connettivi = [token.text for token in doc if token.dep_ in ['cc', 'mark']]
    return connettivi

connettivi_umano= testi_umano['text'].apply(estrai_connettivi)
connettivi_macchina= testi_macchina['text'].apply(estrai_connettivi)

print(connettivi_umano.head())
print(connettivi_macchina.head())

from collections import Counter

# Funzione per calcolare il numero totale di verbi
def numero_verbi(text):
    verbi = estrai_verbi(text)
    return len(verbi)  # Restituisce il numero di verbi

# Funzione per calcolare il numero totale di connettivi
def numero_connettivi(text):
    connettivi = estrai_connettivi(text)
    return len(connettivi)  # Restituisce il numero di connettivi

# Applicazione delle funzioni ai dataset e creazione di colonne con queste metriche
testi_umano['numero_verbi'] = testi_umano['text'].apply(numero_verbi)
testi_macchina['numero_verbi'] = testi_macchina['text'].apply(numero_verbi)

numero_connettivi_umani= testi_umano['text'].apply(numero_connettivi)
numero_connettivi_macchina = testi_macchina['text'].apply(numero_connettivi)

#calcolo frequenza media dei verbi

# Calcolo la frequenza media dei verbi per testo
testi_umano_frequenza_verbi= testi_umano['numero_verbi'].mean()
testi_macchina_frequenza_verbi = testi_macchina['numero_verbi'].mean()

# Salvataggio in un file di testo
with open('frequenza_verbi.txt', 'w') as f:
    f.write(f"Frequenza media dei verbi per i testi umani: {testi_umano_frequenza_verbi}\n")
    f.write(f"Frequenza media dei verbi per i testi generati dalla macchina: {testi_macchina_frequenza_verbi}\n")

files.download('frequenza_verbi.txt')

#creiamo il grafico del confronto del numero medio di verbi tra testi umani e testi macchina

# Dati
labels = ['Testi Umani', 'Testi Macchina']
valori_verbi = [testi_umano_frequenza_verbi, testi_macchina_frequenza_verbi]

# Creazione del grafico a barre
plt.figure(figsize=(8, 5))
plt.bar(labels, valori_verbi, color=['blue', 'green'])

# Aggiungo il titolo e le etichette
plt.title('Confronto numero medio di verbi tra Testi Umani e Testi Macchina')
plt.ylabel('media')

# Mostra il grafico
plt.tight_layout()
plt.savefig('grafico_verbi.png')
files.download('grafico_verbi.png')

# Funzione per calcolare il TTR dei verbi- varietà di verbi usati
def calcola_ttr_verbi(text):
    verbi = estrai_verbi(text)
    num_verbi = len(verbi)  # numero totale di verbi
    num_verbi_unici = len(set(verbi))  # numero di verbi unici
    if num_verbi == 0:
        return 0  # Evita la divisione per zero
    return num_verbi_unici / num_verbi  # TTR dei verbi

# Calcola il TTR per i verbi nei testi umani
testi_umano['TTR verbi'] = testi_umano['text'].apply(calcola_ttr_verbi)

# Calcola il TTR per i verbi nei testi generati dalla macchina
testi_macchina ['TTR verbi'] = testi_macchina['text'].apply(calcola_ttr_verbi)

# Calcola la media del TTR per i verbi

media_ttr_verbi_umano= testi_umano['TTR verbi'].mean()

# Calcolare la media del TTR per i verbi nei testi generati dalla macchina
media_ttr_verbi_macchina = testi_macchina['TTR verbi'].mean()

with open('ttr_verbi.txt', 'w') as f:
    f.write(f"Media TTR dei verbi per i testi generati dalla macchina: {media_ttr_verbi_macchina}\n")
    f.write(f"Media TTR dei verbi per i testi prodotti da umano:{media_ttr_verbi_umano}\n")

files.download('ttr_verbi.txt')

#costruisco il grafico

# Dati per il grafico
labels = ['Verbi umano', 'Verbi macchina']
valori_totali= [media_ttr_verbi_umano, media_ttr_verbi_macchina]

# Creazione del grafico a barre
plt.figure(figsize=(8, 5))
plt.bar(labels, valori_totali, color=['blue', 'green'])


# Aggiungi il titolo e le etichette
plt.title('Confronto TTR medio verbi tra Testi Umani e Testi Macchina')
plt.ylabel('ttr medio')

# Mostra il grafico
plt.tight_layout()
plt.savefig('grafico_ttr_verbi.png')
files.download('grafico_ttr_verbi.png')

#uniamo i due dataset macchina e umano per allenare il nostro modello di machine learning

dataset_completo = pd.concat([testi_umano, testi_macchina], ignore_index=True)

#parte di machine learning e costruzione del mio modello- classificatore

import keras
keras.__version__

#divido i dati in test set, e training set
#faccio questa operazione separando anche le features numeriche dai testi

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Separazione tra testo e metadati (features numeriche)
X_text = dataset_completo['text']
X_metrics = dataset_completo.drop(columns=['text', 'source'])
y = dataset_completo['source']

# Divisione in train, test e validation
X_train_text, X_test_text, X_train_metrics, X_test_metrics, y_train, y_test = train_test_split(
    X_text, X_metrics, y, test_size=0.2, random_state=42)

# Ulteriore divisione per il validation set
X_train_text, X_val_text, X_train_metrics, X_val_metrics, y_train, y_val = train_test_split(
    X_train_text, X_train_metrics, y_train, test_size=0.2, random_state=42)


# Preprocessing delle metriche numeriche (scaling)
#L'obiettivo è fare in modo che tutte le variabili numeriche (features) abbiano la stessa scala, in modo che il modello non venga influenzato da una variabile con valori più grandi o più piccoli
scaler = StandardScaler()
X_train_metrics_scaled = scaler.fit_transform(X_train_metrics)  # Fitting solo sui dati di training
X_val_metrics_scaled = scaler.transform(X_val_metrics)  # Applicazione della trasformazione al validation set
X_test_metrics_scaled = scaler.transform(X_test_metrics)  # Applicazione della trasformazione al test set

import numpy as np

#Converto i testi del mio dataset in sequenze di indici, che darò poi in pasto alla rete neurale che a partire da queste creerà gli embedding

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences #pad_sequences serve a trasformare liste di diverse lunghezze in liste della stessa dimensione

max_features = 10000 #dimensione del vocabolario
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(X_train_text)

vocabolario = tokenizer.word_index
print("Vocabolario:", vocabolario)

X_train_sequencias = tokenizer.texts_to_sequences(X_train_text)
X_val_sequencias = tokenizer.texts_to_sequences(X_val_text)
X_test_sequencias = tokenizer.texts_to_sequences(X_test_text)

maxlen = 80

#Adatta le sequenze alla stessa lunghezza
X_train_sequencias_padded = pad_sequences(X_train_sequencias, padding='post', maxlen=maxlen)
X_val_sequencias_padded = pad_sequences(X_val_sequencias, padding='post', maxlen=maxlen )
X_test_sequencias_padded = pad_sequences(X_test_sequencias, padding='post', maxlen=maxlen)

#Convertiamo le sequenze di addestramento, validazione e test in array di numpy
X_train_sequencias_2 = np.array(X_train_sequencias_padded)
X_val_sequencias_2 = np.array(X_val_sequencias_padded)
X_test_sequencias_2 = np.array(X_test_sequencias_padded)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

#Codifichiamo le etichette delle classi in formato numerico
y_train_etiquetas_codificadas = label_encoder.fit_transform(y_train)
y_val_etiquetas_codificadas = label_encoder.transform(y_val)
y_test_etiquetas_codificadas = label_encoder.transform(y_test)


y_train = np.array(y_train_etiquetas_codificadas)
y_val = np.array(y_val_etiquetas_codificadas)
y_test = np.array(y_test_etiquetas_codificadas)

X_train_metrics_scaled = np.nan_to_num(X_train_metrics_scaled, nan=0.0, posinf=1.0, neginf=-1.0)

X_val_metrics_scaled = np.nan_to_num(X_val_metrics_scaled, nan=0.0, posinf=1.0, neginf=-1.0)

from sklearn.utils import shuffle

# Mescolare i dati di addestramento e di validazione
X_train_sequencias_2, X_train_metrics_scaled, y_train = shuffle(X_train_sequencias_2, X_train_metrics_scaled, y_train)
X_val_sequencias_2, X_val_metrics_scaled, y_val = shuffle(X_val_sequencias_2, X_val_metrics_scaled, y_val)
X_test_sequencias_2, X_test_metrics_scaled, y_test = shuffle(X_test_sequencias_2, X_test_metrics_scaled, y_test)

import os
import random
import numpy as np
import tensorflow as tf

# Fissare il random state - questo perché i miei risultati siano replicabili
seed_value = 42
os.environ['PYTHONHASHSEED'] = str(seed_value)
random.seed(seed_value)
np.random.seed(seed_value)
tf.random.set_seed(seed_value)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ReduceLROnPlateau

# 1. Parametri
maxlen = 80
embedding_dim = 100       # La dimensione degli embedding
num_metrics = X_train_metrics.shape[1]  # Numero di features numeriche
max_features = 10000  # Dimensione del vocabolario

# Input per il testo
text_input = Input(shape=(maxlen,), dtype='int32', name="text_input")

embedding_layer = Embedding(input_dim=max_features, output_dim=embedding_dim)(text_input)

lstm_layer = LSTM(256, return_sequences=False)(embedding_layer)

# Aggiungo un layer di dropout dopo l'LSTM
lstm_layer = Dropout(0.5)(lstm_layer)

# Input per le feature numeriche
numerical_input = Input(shape=(X_train_metrics.shape[1],), dtype='float32', name="numerical_input")

# Concatenazione dei flussi (testo + numerico)
combined = Concatenate()([lstm_layer, numerical_input])

# Fully connected layer
dense_layer = Dense(64, activation='relu', kernel_initializer='glorot_uniform')(combined)

dense_layer = Dropout(0.5)(dense_layer)

# Output layer per la classificazione
output = Dense(1, activation='sigmoid')(dense_layer)

# Creazione del modello
model = Model(inputs=[text_input, numerical_input], outputs=output)

# Calcolo dinamico dei pesi delle classi
class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(zip(np.unique(y_train), class_weights_array))
print(f"Class weights: {class_weights_dict}")

# Compilazione del modello
optimizer = Adam(learning_rate=0.0001, clipvalue=1.0)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# EarlyStopping
early_stopping = EarlyStopping(
    patience=10,
    min_delta=0.01,
    restore_best_weights=True,
    monitor='val_loss'
)

# Callback ReduceLROnPlateau per ridurre il learning rate quando la perdita non migliora
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6,
    verbose=1
)

# Allenamento del modello
history = model.fit([X_train_sequencias_2, X_train_metrics_scaled], y_train,
    epochs=30,
    batch_size=32,
    validation_data=([X_val_sequencias_2, X_val_metrics_scaled], y_val),
    callbacks=[early_stopping, reduce_lr],
    class_weight=class_weights_dict,
    verbose=2,
    shuffle=True,
)

X_test_metrics_scaled = np.nan_to_num(X_test_metrics_scaled, nan=0.0, posinf=1.0, neginf=-1.0)

score = model.evaluate([X_test_sequencias_2, X_test_metrics_scaled], y_test, verbose=1)
# Salva l'accuratezza in un file di testo
with open('accuracy_model.txt', 'w') as f:
    f.write(f"Accuracy del modello: {score[1]:.4f}\n")

files.download('accuracy_model.txt')

# Questo codice visualizza i grafici dell'accuratezza e della perdita durante l'addestramento e la validazione del modello.
import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.savefig('grafico_accuratezza_loss.png')
files.download('grafico_accuratezza_loss.png')

from sklearn.metrics import confusion_matrix
import numpy as np

# Previsione delle classi
y_pred = model.predict([X_test_sequencias_2, X_test_metrics_scaled])

# Applica una soglia di 0.5 per ottenere le classi (0 o 1)
y_pred_classes = (y_pred > 0.5).astype(int)

# Calcola la matrice di confusione
cm = confusion_matrix(y_test, y_pred_classes)

#creiamo il grafico della matrice di confusione

import seaborn as sns

cm = confusion_matrix(y_test, y_pred_classes)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=True, yticklabels=True)

plt.xlabel('Classe Predetta')
plt.ylabel('Classe Reale')
plt.title('Matrice di Confusione')

plt.savefig('matrice_confusione.png')
files.download('matrice_confusione.png')

from sklearn.metrics import classification_report

#Stampiamo il report di classificazione, che include metriche come precisione, recall e F1-score per ogni classe
report= classification_report(y_test, y_pred_classes, target_names=['ai', 'human'], output_dict=True)

# Convertire il report in un DataFrame
report_df = pd.DataFrame(report).transpose()

# Salvarlo come CSV
report_df.to_csv('classification_report.csv', index=True)

files.download('classification_report.csv')

#Applicazione di LIME per spiegare l'influenza delle diverse feature sulle decisioni del classificatore
# In questa sezione utilizziamo LimeTabularExplainer per analizzare come le feature numeriche contribuiscono alle predizioni del modello. Questo aiuta a interpretare meglio il comportamento del classificatore
# e a identificare le caratteristiche più rilevanti per ogni classe
!pip install lime

from lime.lime_text import LimeTextExplainer
from lime.lime_tabular import LimeTabularExplainer

from sklearn.feature_extraction.text import TfidfVectorizer

def preprocess(texts, metrics):
    # Converti i testi in sequenze numeriche (usando il tokenizer pre-addestrato)
    text_sequences = tokenizer.texts_to_sequences(texts)
    text_padded = tf.keras.preprocessing.sequence.pad_sequences(text_sequences, maxlen=maxlen)

    # Normalizza le metriche numeriche (usando lo scaler pre-addestrato)
    metrics_normalized = scaler.transform(metrics)

    return [np.array(text_padded), np.array(metrics_normalized)]

def predict_proba_lime(texts):
    # Supponendo che `metrics` sia un array di feature numeriche associate ai testi
    metrics = np.zeros((len(texts), X_train_metrics.shape[1]))

    # Preprocessa i dati
    preprocessed_data = preprocess(texts, metrics)

    # Ottieni le probabilità dal modello
    proba_positive = model.predict(preprocessed_data).flatten()

    # Calcola la probabilità della classe negativa
    proba_negative = 1 - proba_positive

    # Combina entrambe le probabilità in una matrice (n_samples, n_classes)
    probabilities = np.vstack((proba_negative, proba_positive)).T

    return probabilities

# Creazione dell'esploratore LIME
explainer = LimeTextExplainer(class_names= ["ai", "human"])

# Testo di esempio da spiegare
test_text = dataset_completo["text"]

# Genera la spiegazione
explanation = explainer.explain_instance(
    test_text[0],         # Il testo da analizzare
    predict_proba_lime,         # La funzione predittiva definita sopra
    num_features=3       # Numero di caratteristiche importanti da mostrare
)

import csv

# Ottieni la lista delle caratteristiche più influenti
caratteristiche_influenti = explanation.as_list()

# Salva in un file CSV
with open('caratteristiche_influenti.csv', mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Caratteristica', 'Peso'])
    writer.writerows(caratteristiche_influenti)
files.download('caratteristiche_influenti.csv')

explanation = explainer.explain_instance(
    test_text[60],         # Il testo da analizzare
    predict_proba_lime,         # La funzione predittiva definita sopra
    num_features=3       # Numero di caratteristiche importanti da mostrare
)

# Visualizza la spiegazione
explanation.show_in_notebook()

# Mostra la lista delle caratteristiche più influenti
caratteristiche_influenti_esempio_due=explanation.as_list()

with open('caratteristiche_influenti_esempio_due.csv', mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Caratteristica', 'Peso'])
    writer.writerows(caratteristiche_influenti_esempio_due)
files.download('caratteristiche_influenti_esempio_due.csv')

from lime.lime_tabular import LimeTabularExplainer
import numpy as np


training_data_array = X_train_metrics.values

feature_names = X_train_metrics.columns.tolist()


explainer = LimeTabularExplainer(
    training_data=training_data_array,
    mode='classification',
    class_names=["ai", "human"],
    feature_names=feature_names,
    discretize_continuous=True
)

def predict_proba_lime(features):
    features = np.array(features)

    if features.ndim == 1:
        features = features.reshape(1, -1)

    #Crea un input di testo fittizio (tutti zeri) poiché il modello si aspetta due input
    dummy_text_input = np.zeros((features.shape[0], maxlen), dtype='int32')

    probabilities = model.predict([dummy_text_input, features])

    return np.hstack([(1 - probabilities), probabilities])

# Seleziona un campione di test da spiegare
sample = X_test_metrics.iloc[0]

# spiegazione della predizione
explanation = explainer.explain_instance(
    sample,  # Dati per la predizione
    predict_proba_lime,
    num_features=5  # Numero di feature da visualizzare
)

# Mostra la spiegazione
feature_importanti= explanation.as_list()
with open('feature_importanti.csv', mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Caratteristica', 'Peso'])
    writer.writerows(feature_importanti)
files.download('feature_importanti.csv')

nuovo_esempio= X_test_metrics.iloc[60]

explanation = explainer.explain_instance(
    nuovo_esempio,  # Dati per la predizione
    predict_proba_lime,
    num_features=5  # Numero di feature da visualizzare
)

feature_importanti_2=explanation.as_list()
with open('feature_importanti_2.csv', mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Caratteristica', 'Peso'])
    writer.writerows(feature_importanti_2)
files.download('feature_importanti_2.csv')